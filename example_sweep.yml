program: ddqn_main.py
method: grid
metric:
  goal: maximize
  name: final_score
parameters:
  env_name:
    values: ['CartPole-v1'] #'CartPole-v1', 'Acrobot-v1'
  agent_type:
    values: ['rand_delayed']
#  use_learned_forward_model:
#    values: [False] #, True]
  delay_value:
    values: [15] #,0 15, 25]
#  physical_noise_std_ratio:
#    values: [0.1]
  seed:
    values: [2, 3] #, 2, 3]
#  use_reward_shaping:
#    values: [True]
#  epsilon_decay:
#    values: [0.999] # 0.9999 for acrobot
#  epsilon_min:
#    values: [0.001]
#  learning_rate:
#    values: [0.005]
#  double_q:
#    values: [True]
#  target_network_update_freq:
#    values: [300]
  total_steps:
    values: [100000] #000]
  walking_p:
    values: [0, 0.01, 0.05, 0.1, 0.2] #, 0.3]

